{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Qp0H_zUQuu_"
   },
   "source": [
    "# Нейронные сети\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "__Решение отправлять на `ml.course.practice@gmail.com`__\n",
    "\n",
    "__Тема письма: `[HSE][ML][MS][HW05] <ФИ>`, где вместо `<ФИ>` указаны фамилия и имя__\n",
    "\n",
    "Для начала вам предстоит реализовать свой собственный backpropagation и протестировать его на реальных данных, а затем научиться обучать нейронные сети при помощи библиотеки `PyTorch` и использовать это умение для классификации классического набора данных CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "22ezVRf3QuvA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from typing import List, NoReturn\n",
    "from math import exp, log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qfDPH_LQuvF"
   },
   "source": [
    "### Задание 1 (3 балла)\n",
    "Нейронные сети состоят из слоев, поэтому для начала понадобится реализовать их. Пока нам понадобятся только три:\n",
    "\n",
    "`Linear` - полносвязный слой, в котором `y = Wx + b`, где `y` - выход, `x` - вход, `W` - матрица весов, а `b` - смещение. \n",
    "\n",
    "`ReLU` - слой, соответствующий функции активации `y = max(0, x)`.\n",
    "\n",
    "`Softmax` - слой, соответствующий функции активации [softmax](https://ru.wikipedia.org/wiki/Softmax)\n",
    "\n",
    "\n",
    "#### Методы\n",
    "`forward(X)` - возвращает предсказанные для `X`. `X` может быть как вектором, так и батчем\n",
    "\n",
    "`backward(d)` - считает градиент при помощи обратного распространения ошибки. Возвращает новое значение `d`\n",
    "\n",
    "`update(alpha)` - обновляет веса (если необходимо) с заданой скоростью обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "RWFLlHqaYbgC"
   },
   "outputs": [],
   "source": [
    "class Module:\n",
    "    \"\"\"\n",
    "    Абстрактный класс. Его менять не нужно.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def backward(self, d):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def params(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "94hkbnD1QuvG"
   },
   "outputs": [],
   "source": [
    "def l2_regularization(W, reg_strength):\n",
    "    \"\"\"\n",
    "    Computes L2 regularization loss on weights and its gradient\n",
    "\n",
    "    Arguments:\n",
    "      W, np array - weights\n",
    "      reg_strength - float value\n",
    "\n",
    "    Returns:\n",
    "      loss, single value - l2 regularization loss\n",
    "      gradient, np.array same shape as W - gradient of weight by l2 loss\n",
    "    \"\"\"\n",
    "    # TODO: Copy from the previous assignment\n",
    "    loss = reg_strength*sum(sum(W**2));\n",
    "    grad = reg_strength*2*W;\n",
    "    \n",
    "    return loss, grad\n",
    "\n",
    "def cross_entropy_loss(probs, target_index):\n",
    "    '''\n",
    "    Computes cross-entropy loss\n",
    "\n",
    "    Arguments:\n",
    "      probs, np array, shape is either (N) or (batch_size, N) -\n",
    "        probabilities for every class\n",
    "      target_index: np array of int, shape is (1) or (batch_size) -\n",
    "        index of the true class for given sample(s)\n",
    "\n",
    "    Returns:\n",
    "      loss: single value\n",
    "    '''\n",
    "    # TODO implement cross-entropy\n",
    "    #print(\"probs:\", probs);\n",
    "    \n",
    "    return -log(probs[target_index - 1]);\n",
    "\n",
    "\n",
    "def softmax_with_cross_entropy(predictions, target_index):\n",
    "    \"\"\"\n",
    "    Computes softmax and cross-entropy loss for model predictions,\n",
    "    including the gradient\n",
    "\n",
    "    Arguments:\n",
    "      predictions, np array, shape is either (N) or (N, batch_size) -\n",
    "        classifier output\n",
    "      target_index: np array of int, shape is (1) or (batch_size) -\n",
    "        index of the true class for given sample(s)\n",
    "\n",
    "    Returns:\n",
    "      loss, single value - cross-entropy loss\n",
    "      dprediction, np array same shape as predictions - gradient of predictions by loss value\n",
    "    \"\"\"\n",
    "    # TODO: Copy from the previous assignment\n",
    "   # TODO implement softmax with cross-entropy\n",
    "    \n",
    "    #One-dimension option\n",
    "    \n",
    "    if predictions.ndim == 1:\n",
    "        predictions_ = predictions - np.max(predictions);\n",
    "        dprediction = np.array(list(map(exp, predictions_)));\n",
    "        summ = sum(dprediction);\n",
    "        dprediction /= summ;\n",
    "        \n",
    "        loss = cross_entropy_loss(dprediction, target_index);\n",
    "        dprediction[target_index - 1] -= 1;\n",
    "        \n",
    "        return loss, dprediction;\n",
    "    else:\n",
    "    \n",
    "        predictions_ = predictions - np.max(predictions, axis = 1)[:, np.newaxis];\n",
    "        exp_vec = np.vectorize(exp);\n",
    "        #print(\"predictions_:\", predictions_);\n",
    "        \n",
    "        dprediction = np.apply_along_axis(exp_vec, 1, predictions_);\n",
    "        #print(\"dprediction before division: \", dprediction);\n",
    "    \n",
    "        summ = sum(dprediction.T);\n",
    "        #print(\"summ: \", summ);\n",
    "        dprediction /= summ[:, np.newaxis];\n",
    "            \n",
    "        #print(\"dprediction after division: \", dprediction);\n",
    "    \n",
    "        loss = np.array([cross_entropy_loss(x,y) for x,y in zip(dprediction, target_index)]);\n",
    "        #print(\"loss: \", loss);\n",
    "        \n",
    "        #print(\"target_index - 1:\", target_index - 1);\n",
    "        it = np.nditer(target_index - 1, flags = ['c_index'] )\n",
    "        while not it.finished:\n",
    "            #print(\"it[0] = \", it[0]);\n",
    "            dprediction[it.index, it[0]] -= 1\n",
    "            it.iternext()\n",
    "        \n",
    "        dprediction /= len(target_index);\n",
    "        #print(\"dprediction after subtraction: \", dprediction);\n",
    "    \n",
    "        return loss.mean(), dprediction;\n",
    "    raise Exception(\"Not implemented!\")\n",
    "\n",
    "\n",
    "class Param:\n",
    "    \"\"\"\n",
    "    Trainable parameter of the model\n",
    "    Captures both parameter value and the gradient\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value):\n",
    "    #self.init = value.copy();\n",
    "        self.value = value;\n",
    "        self.grad = np.zeros_like(value);\n",
    "\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # TODO: Implement forward pass\n",
    "        # Hint: you'll need to save some information about X\n",
    "        # to use it later in the backward pass\n",
    "        self.X = X;\n",
    "        return (X > 0)*X;\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "\n",
    "        Arguments:\n",
    "        d_out, np array (batch_size, num_features) - gradient\n",
    "           of loss function with respect to output\n",
    "\n",
    "        Returns:\n",
    "        d_result: np array (batch_size, num_features) - gradient\n",
    "          with respect to input\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Implement backward pass\n",
    "        # Your final implementation shouldn't have any loops\n",
    "        \n",
    "        return (self.X > 0)*d_out;\n",
    "\n",
    "    def params(self):\n",
    "        # ReLU Doesn't have any parameters\n",
    "        return {}\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        self.W = Param(0.01 * np.random.randn(n_input, n_output))\n",
    "        self.B = Param(0.01 * np.random.randn(1, n_output))\n",
    "        self.X = None\n",
    "        self.n_output = n_output\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # TODO: Implement forward pass\n",
    "        # Your final implementation shouldn't have any loops\n",
    "        self.X = X;\n",
    "        #if np.any(self.W.init != self.W.value) or np.any(self.B.init != self.B.value):\n",
    "        self.W.grad = np.zeros_like(self.W.value);\n",
    "        self.B.grad = np.zeros_like(self.B.value);\n",
    "        #    self.W.init = self.W.value;\n",
    "        #    self.B.init = self.B.value;\n",
    "        return np.dot(self.X, self.W.value) + self.B.value;\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "        Computes gradient with respect to input and\n",
    "        accumulates gradients within self.W and self.B\n",
    "\n",
    "        Arguments:\n",
    "        d_out, np array (batch_size, n_output) - gradient\n",
    "           of loss function with respect to output\n",
    "\n",
    "        Returns:\n",
    "        d_result: np array (batch_size, n_input) - gradient\n",
    "          with respect to input\n",
    "        \"\"\"\n",
    "        # TODO: Implement backward pass\n",
    "        # Compute both gradient with respect to input\n",
    "        # and gradients with respect to W and B\n",
    "        # Add gradients of W and B to their `grad` attribute\n",
    "\n",
    "        # It should be pretty similar to linear classifier from\n",
    "        # the previous assignment\n",
    "        \n",
    "        dW = np.dot(self.X.T, d_out);\n",
    "        dB = np.dot(np.ones((1, d_out.shape[0])), d_out);\n",
    "        \n",
    "        d_input = np.dot(d_out, self.W.value.T);\n",
    "        \n",
    "        self.W.grad += dW;\n",
    "        self.B.grad += dB;\n",
    "        \n",
    "        return d_input;\n",
    "\n",
    "    def params(self):\n",
    "        return {'W': self.W, 'B': self.B}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb_ip_h8QuvJ"
   },
   "source": [
    "### Задание 2 (2 балла)\n",
    "Теперь сделаем саму нейронную сеть.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - обучает нейронную сеть заданное число эпох. В каждой эпохе необходимо использовать [cross-entropy loss](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy) для обучения, а так же производить обновления не по одному элементу, а используя батчи.\n",
    "\n",
    "`predict_proba(X)` - предсказывает вероятности классов для элементов `X`\n",
    "\n",
    "#### Параметры конструктора\n",
    "`modules` - список, состоящий из ранее реализованных модулей и описывающий слои нейронной сети. В конец необходимо добавить `Softmax`\n",
    "\n",
    "`epochs` - количество эпох обучения\n",
    "\n",
    "`alpha` - скорость обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Q_JFCizKQuvK"
   },
   "outputs": [],
   "source": [
    "class MLPClassifier:\n",
    "    def __init__(self, modules: List[Module], reg = 2e-3):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        modules : List[Module]\n",
    "            Cписок, состоящий из ранее реализованных модулей и \n",
    "            описывающий слои нейронной сети. \n",
    "            В конец необходимо добавить Softmax.\n",
    "        reg, float - L2 regularization strength\n",
    "        \"\"\"\n",
    "        self.reg = reg\n",
    "        # TODO Create necessary layers\n",
    "        self.modules = modules\n",
    "        self.n_output = self.modules[-1].n_output\n",
    "        \n",
    "    def compute_loss_and_gradients(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes total loss and updates parameter gradients\n",
    "        on a batch of training examples\n",
    "\n",
    "        Arguments:\n",
    "        X, np array (batch_size, input_features) - input data\n",
    "        y, np array of int (batch_size) - classes\n",
    "        \"\"\"\n",
    "        # Before running forward and backward pass through the model,\n",
    "        # clear parameter gradients aggregated from the previous pass\n",
    "        # TODO Set parameter gradient to zeros\n",
    "        # Hint: using self.params() might be useful!\n",
    "        for param in self.params():\n",
    "            self.params()[param].grad = np.zeros_like(self.params()[param].grad);\n",
    "        \n",
    "        # TODO Compute loss and fill param gradients\n",
    "        # by running forward and backward passes through the model\n",
    "        predictions = X\n",
    "        for module in self.modules:\n",
    "            predictions = module.forward(predictions)\n",
    "        \n",
    "        loss, dX = softmax_with_cross_entropy(predictions, y + 1);\n",
    "        for i in range(len(self.modules)-1, -1, -1):\n",
    "            dX = self.modules[i].backward(dX)\n",
    "        \n",
    "        # After that, implement l2 regularization on all params\n",
    "        # Hint: self.params() is useful again!\n",
    "        for param in self.params():\n",
    "            if(param[0] == 'W'):\n",
    "                loss_, grad_ = l2_regularization(self.params()[param].value, self.reg);\n",
    "                self.params()[param].grad += grad_;\n",
    "                loss += loss_;\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Produces classifier predictions on the set\n",
    "\n",
    "        Arguments:\n",
    "          X, np array (test_samples, num_features)\n",
    "\n",
    "        Returns:\n",
    "          y_pred, np.array of int (test_samples)\n",
    "        \"\"\"\n",
    "        # TODO: Implement predict\n",
    "        # Hint: some of the code of the compute_loss_and_gradients\n",
    "        # can be reused\n",
    "        pred = np.zeros(X.shape[0], np.int)\n",
    "        predictions = X\n",
    "        for module in self.modules:\n",
    "            predictions = module.forward(predictions)\n",
    "        i=0;\n",
    "        for predict in predictions:\n",
    "            values = [softmax_with_cross_entropy(predict, target_index + 1)[0] \\\n",
    "                        for target_index in range(self.n_output)];\n",
    "            pred[i] = min(range(len(values)), key=values.__getitem__);\n",
    "            i += 1;\n",
    "        return pred\n",
    "\n",
    "    def params(self):\n",
    "        result = {}\n",
    "        # TODO Implement aggregating all of the params\n",
    "        i = 1\n",
    "        for module in self.modules:\n",
    "            dict_module = module.params()\n",
    "            if len(dict_module) > 0:\n",
    "                for key in dict_module.keys():\n",
    "                    result[key + str(i)] = dict_module[key];\n",
    "                i += 1\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5526/3139534895.py:65: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  pred = np.zeros(X.shape[0], np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.693416, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.692531, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.692054, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691788, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691635, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691544, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691488, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691453, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691430, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691414, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691403, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691395, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691389, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691384, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691381, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691378, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691375, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691373, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691371, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691370, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691368, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691367, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691366, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691365, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691364, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691363, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691362, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691361, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691360, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691360, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691359, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691358, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691358, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691357, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691356, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691356, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691355, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691355, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691354, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691354, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691354, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691353, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691353, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691353, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691352, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691352, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691352, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691351, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691351, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691351, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691351, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691350, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691350, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691350, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691350, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691350, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691349, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691349, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691349, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691349, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691349, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691349, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691349, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691349, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691348, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691348, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691348, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691348, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691348, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691348, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691348, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691348, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691348, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691348, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691348, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n",
      "Loss: 0.691347, Train accuracy: 0.533333, val accuracy: 0.400000\n"
     ]
    }
   ],
   "source": [
    "from optim import SGD\n",
    "from trainer import Trainer, Dataset\n",
    "from dataset import random_split_train_val\n",
    "\n",
    "model = MLPClassifier([\n",
    "    Linear(4, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 32),\n",
    "    ReLU(),\n",
    "    Linear(32, 2)\n",
    "])\n",
    "\n",
    "X = np.random.randn(50, 4)\n",
    "y = np.array([(0 if x[0] > x[2]**2 or x[3]**3 > 0.5 else 1) for x in X])\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(X, y, num_val = 20)\n",
    "\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y);\n",
    "trainer = Trainer(model, dataset, SGD(),  num_epochs=100, batch_size=100,\n",
    "                  learning_rate=5e-1, learning_rate_decay= 0.95);\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C1EIsDqQuvQ"
   },
   "source": [
    "### Задание 3 (2 балла)\n",
    "Протестируем наше решение на синтетических данных. Необходимо подобрать гиперпараметры, при которых качество полученных классификаторов будет достаточным.\n",
    "\n",
    "#### Оценка\n",
    "Accuracy на первом датасете больше 0.85 - +1 балл\n",
    "\n",
    "Accuracy на втором датасете больше 0.85 - +1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "d5UAgXTcQuvQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.03433496  0.04105682]\n",
      " [ 1.76605771 -0.18736136]\n",
      " [ 1.61936452 -0.25047369]\n",
      " [-0.9247644   0.56075625]\n",
      " [ 1.16215654  0.10360873]\n",
      " [ 1.74026995  0.0213692 ]\n",
      " [ 2.15053752  0.18210805]\n",
      " [ 0.21958816  0.01136422]\n",
      " [ 0.63353984  0.80059859]\n",
      " [ 1.81320861  0.26595535]]\n",
      "[0 1 1 0 0 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "X, y = make_moons(400, noise=0.075)\n",
    "X_test, y_test = make_moons(400, noise=0.075)\n",
    "\n",
    "print(X[:10])\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5526/3139534895.py:65: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  pred = np.zeros(X.shape[0], np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.692768, Train accuracy: 0.500000, val accuracy: 0.500000\n",
      "Loss: 0.692327, Train accuracy: 0.500000, val accuracy: 0.500000\n",
      "Loss: 0.692669, Train accuracy: 0.785000, val accuracy: 0.787500\n",
      "Loss: 0.691610, Train accuracy: 0.787500, val accuracy: 0.797500\n",
      "Loss: 0.691548, Train accuracy: 0.792500, val accuracy: 0.802500\n",
      "Loss: 0.690614, Train accuracy: 0.820000, val accuracy: 0.815000\n",
      "Loss: 0.691822, Train accuracy: 0.785000, val accuracy: 0.787500\n",
      "Loss: 0.692224, Train accuracy: 0.767500, val accuracy: 0.772500\n",
      "Loss: 0.691370, Train accuracy: 0.795000, val accuracy: 0.792500\n",
      "Loss: 0.688584, Train accuracy: 0.807500, val accuracy: 0.810000\n",
      "Loss: 0.688053, Train accuracy: 0.792500, val accuracy: 0.810000\n",
      "Loss: 0.684005, Train accuracy: 0.790000, val accuracy: 0.792500\n",
      "Loss: 0.682429, Train accuracy: 0.795000, val accuracy: 0.807500\n",
      "Loss: 0.682079, Train accuracy: 0.795000, val accuracy: 0.802500\n",
      "Loss: 0.681011, Train accuracy: 0.797500, val accuracy: 0.795000\n",
      "Loss: 0.681840, Train accuracy: 0.797500, val accuracy: 0.787500\n",
      "Loss: 0.673260, Train accuracy: 0.800000, val accuracy: 0.802500\n",
      "Loss: 0.668331, Train accuracy: 0.795000, val accuracy: 0.795000\n",
      "Loss: 0.661562, Train accuracy: 0.797500, val accuracy: 0.797500\n",
      "Loss: 0.651544, Train accuracy: 0.792500, val accuracy: 0.795000\n",
      "Loss: 0.667487, Train accuracy: 0.792500, val accuracy: 0.795000\n",
      "Loss: 0.662363, Train accuracy: 0.797500, val accuracy: 0.800000\n",
      "Loss: 0.645693, Train accuracy: 0.795000, val accuracy: 0.802500\n",
      "Loss: 0.623771, Train accuracy: 0.795000, val accuracy: 0.797500\n",
      "Loss: 0.622533, Train accuracy: 0.795000, val accuracy: 0.797500\n",
      "Loss: 0.618207, Train accuracy: 0.797500, val accuracy: 0.800000\n",
      "Loss: 0.612937, Train accuracy: 0.797500, val accuracy: 0.805000\n",
      "Loss: 0.582557, Train accuracy: 0.800000, val accuracy: 0.802500\n",
      "Loss: 0.605692, Train accuracy: 0.800000, val accuracy: 0.807500\n",
      "Loss: 0.545814, Train accuracy: 0.800000, val accuracy: 0.805000\n",
      "Loss: 0.516331, Train accuracy: 0.800000, val accuracy: 0.805000\n",
      "Loss: 0.536550, Train accuracy: 0.800000, val accuracy: 0.807500\n",
      "Loss: 0.531098, Train accuracy: 0.802500, val accuracy: 0.797500\n",
      "Loss: 0.572496, Train accuracy: 0.807500, val accuracy: 0.805000\n",
      "Loss: 0.514466, Train accuracy: 0.807500, val accuracy: 0.805000\n",
      "Loss: 0.414407, Train accuracy: 0.810000, val accuracy: 0.805000\n",
      "Loss: 0.473007, Train accuracy: 0.815000, val accuracy: 0.807500\n",
      "Loss: 0.517301, Train accuracy: 0.812500, val accuracy: 0.810000\n",
      "Loss: 0.481563, Train accuracy: 0.817500, val accuracy: 0.807500\n",
      "Loss: 0.523258, Train accuracy: 0.817500, val accuracy: 0.810000\n",
      "Loss: 0.416288, Train accuracy: 0.822500, val accuracy: 0.810000\n",
      "Loss: 0.426402, Train accuracy: 0.825000, val accuracy: 0.810000\n",
      "Loss: 0.290895, Train accuracy: 0.822500, val accuracy: 0.810000\n",
      "Loss: 0.366722, Train accuracy: 0.822500, val accuracy: 0.822500\n",
      "Loss: 0.426778, Train accuracy: 0.835000, val accuracy: 0.827500\n",
      "Loss: 0.337618, Train accuracy: 0.835000, val accuracy: 0.832500\n",
      "Loss: 0.373306, Train accuracy: 0.837500, val accuracy: 0.832500\n",
      "Loss: 0.304135, Train accuracy: 0.837500, val accuracy: 0.832500\n",
      "Loss: 0.327831, Train accuracy: 0.837500, val accuracy: 0.832500\n",
      "Loss: 0.343329, Train accuracy: 0.840000, val accuracy: 0.837500\n",
      "Loss: 0.500042, Train accuracy: 0.840000, val accuracy: 0.837500\n",
      "Loss: 0.346481, Train accuracy: 0.842500, val accuracy: 0.840000\n",
      "Loss: 0.346478, Train accuracy: 0.842500, val accuracy: 0.840000\n",
      "Loss: 0.242556, Train accuracy: 0.842500, val accuracy: 0.845000\n",
      "Loss: 0.241213, Train accuracy: 0.840000, val accuracy: 0.845000\n",
      "Loss: 0.363364, Train accuracy: 0.842500, val accuracy: 0.845000\n",
      "Loss: 0.290605, Train accuracy: 0.845000, val accuracy: 0.847500\n",
      "Loss: 0.383711, Train accuracy: 0.845000, val accuracy: 0.847500\n",
      "Loss: 0.379194, Train accuracy: 0.845000, val accuracy: 0.847500\n",
      "Loss: 0.321112, Train accuracy: 0.845000, val accuracy: 0.850000\n",
      "Loss: 0.257393, Train accuracy: 0.845000, val accuracy: 0.847500\n",
      "Loss: 0.306271, Train accuracy: 0.845000, val accuracy: 0.847500\n",
      "Loss: 0.391082, Train accuracy: 0.845000, val accuracy: 0.847500\n",
      "Loss: 0.630818, Train accuracy: 0.845000, val accuracy: 0.847500\n",
      "Loss: 0.354290, Train accuracy: 0.852500, val accuracy: 0.847500\n",
      "Loss: 0.277533, Train accuracy: 0.850000, val accuracy: 0.847500\n",
      "Loss: 0.386353, Train accuracy: 0.855000, val accuracy: 0.847500\n",
      "Loss: 0.261104, Train accuracy: 0.850000, val accuracy: 0.855000\n",
      "Loss: 0.213784, Train accuracy: 0.850000, val accuracy: 0.857500\n",
      "Loss: 0.402238, Train accuracy: 0.850000, val accuracy: 0.860000\n",
      "Loss: 0.278913, Train accuracy: 0.852500, val accuracy: 0.860000\n",
      "Loss: 0.403867, Train accuracy: 0.850000, val accuracy: 0.860000\n",
      "Loss: 0.298052, Train accuracy: 0.855000, val accuracy: 0.860000\n",
      "Loss: 0.351820, Train accuracy: 0.855000, val accuracy: 0.860000\n",
      "Loss: 0.363535, Train accuracy: 0.855000, val accuracy: 0.860000\n",
      "Loss: 0.285335, Train accuracy: 0.855000, val accuracy: 0.860000\n",
      "Loss: 0.362710, Train accuracy: 0.857500, val accuracy: 0.860000\n",
      "Loss: 0.635267, Train accuracy: 0.857500, val accuracy: 0.860000\n",
      "Loss: 0.270476, Train accuracy: 0.857500, val accuracy: 0.860000\n",
      "Loss: 0.254930, Train accuracy: 0.860000, val accuracy: 0.860000\n",
      "Loss: 0.171872, Train accuracy: 0.860000, val accuracy: 0.860000\n",
      "Loss: 0.166790, Train accuracy: 0.860000, val accuracy: 0.860000\n",
      "Loss: 0.200616, Train accuracy: 0.860000, val accuracy: 0.865000\n",
      "Loss: 0.331882, Train accuracy: 0.862500, val accuracy: 0.865000\n",
      "Loss: 0.474480, Train accuracy: 0.865000, val accuracy: 0.867500\n",
      "Loss: 0.224100, Train accuracy: 0.865000, val accuracy: 0.867500\n",
      "Loss: 0.293128, Train accuracy: 0.862500, val accuracy: 0.865000\n",
      "Loss: 0.546687, Train accuracy: 0.867500, val accuracy: 0.867500\n",
      "Loss: 0.149730, Train accuracy: 0.867500, val accuracy: 0.867500\n",
      "Loss: 0.432448, Train accuracy: 0.867500, val accuracy: 0.872500\n",
      "Loss: 0.268789, Train accuracy: 0.867500, val accuracy: 0.872500\n",
      "Loss: 0.195943, Train accuracy: 0.867500, val accuracy: 0.872500\n",
      "Loss: 0.434907, Train accuracy: 0.867500, val accuracy: 0.875000\n",
      "Loss: 0.148892, Train accuracy: 0.867500, val accuracy: 0.872500\n",
      "Loss: 0.261600, Train accuracy: 0.870000, val accuracy: 0.875000\n",
      "Loss: 0.192746, Train accuracy: 0.870000, val accuracy: 0.875000\n",
      "Loss: 0.131301, Train accuracy: 0.870000, val accuracy: 0.875000\n",
      "Loss: 0.441480, Train accuracy: 0.870000, val accuracy: 0.875000\n",
      "Loss: 0.514769, Train accuracy: 0.870000, val accuracy: 0.875000\n",
      "Loss: 0.359126, Train accuracy: 0.870000, val accuracy: 0.875000\n",
      "Accuracy 0.875\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "model = MLPClassifier([\n",
    "    Linear(2, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 2)\n",
    "])\n",
    "dataset = Dataset(X, y, X_test, y_test);\n",
    "trainer = Trainer(model, dataset, SGD(), num_epochs=100, batch_size=64,\n",
    "                  learning_rate=5e-2, learning_rate_decay=0.99);\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "best_acc = max(np.mean(model.predict(X_test) == y_test), best_acc)\n",
    "print(\"Accuracy\", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(400, 2, centers=[[0, 0], [2.5, 2.5], [-2.5, 3]])\n",
    "X_test, y_test = make_blobs(400, 2, centers=[[0, 0], [2.5, 2.5], [-2.5, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "MMDJM4qFQuvT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5526/3139534895.py:65: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  pred = np.zeros(X.shape[0], np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.094659, Train accuracy: 0.332500, val accuracy: 0.332500\n",
      "Loss: 1.092625, Train accuracy: 0.332500, val accuracy: 0.332500\n",
      "Loss: 1.103926, Train accuracy: 0.770000, val accuracy: 0.762500\n",
      "Loss: 1.076684, Train accuracy: 0.720000, val accuracy: 0.717500\n",
      "Loss: 1.068168, Train accuracy: 0.737500, val accuracy: 0.727500\n",
      "Loss: 1.035866, Train accuracy: 0.707500, val accuracy: 0.707500\n",
      "Loss: 1.006966, Train accuracy: 0.717500, val accuracy: 0.717500\n",
      "Loss: 0.984552, Train accuracy: 0.755000, val accuracy: 0.760000\n",
      "Loss: 0.850905, Train accuracy: 0.750000, val accuracy: 0.752500\n",
      "Loss: 1.046216, Train accuracy: 0.780000, val accuracy: 0.777500\n",
      "Loss: 0.741381, Train accuracy: 0.780000, val accuracy: 0.777500\n",
      "Loss: 0.676928, Train accuracy: 0.780000, val accuracy: 0.775000\n",
      "Loss: 0.524024, Train accuracy: 0.780000, val accuracy: 0.780000\n",
      "Loss: 0.596873, Train accuracy: 0.790000, val accuracy: 0.790000\n",
      "Loss: 0.910069, Train accuracy: 0.807500, val accuracy: 0.800000\n",
      "Loss: 0.365530, Train accuracy: 0.815000, val accuracy: 0.805000\n",
      "Loss: 0.599554, Train accuracy: 0.822500, val accuracy: 0.815000\n",
      "Loss: 0.417669, Train accuracy: 0.830000, val accuracy: 0.825000\n",
      "Loss: 0.437754, Train accuracy: 0.840000, val accuracy: 0.830000\n",
      "Loss: 0.414079, Train accuracy: 0.842500, val accuracy: 0.840000\n",
      "Loss: 0.496879, Train accuracy: 0.865000, val accuracy: 0.860000\n",
      "Loss: 0.324628, Train accuracy: 0.870000, val accuracy: 0.860000\n",
      "Loss: 0.369594, Train accuracy: 0.872500, val accuracy: 0.867500\n",
      "Loss: 0.613628, Train accuracy: 0.887500, val accuracy: 0.870000\n",
      "Loss: 0.353892, Train accuracy: 0.887500, val accuracy: 0.875000\n",
      "Loss: 0.165312, Train accuracy: 0.887500, val accuracy: 0.875000\n",
      "Loss: 0.410735, Train accuracy: 0.900000, val accuracy: 0.885000\n",
      "Loss: 0.355289, Train accuracy: 0.905000, val accuracy: 0.887500\n",
      "Loss: 0.305764, Train accuracy: 0.905000, val accuracy: 0.887500\n",
      "Loss: 0.397054, Train accuracy: 0.907500, val accuracy: 0.887500\n",
      "Loss: 0.319247, Train accuracy: 0.907500, val accuracy: 0.887500\n",
      "Loss: 0.395008, Train accuracy: 0.910000, val accuracy: 0.890000\n",
      "Loss: 0.531259, Train accuracy: 0.912500, val accuracy: 0.897500\n",
      "Loss: 0.554349, Train accuracy: 0.917500, val accuracy: 0.902500\n",
      "Loss: 0.182062, Train accuracy: 0.915000, val accuracy: 0.905000\n",
      "Loss: 0.346841, Train accuracy: 0.915000, val accuracy: 0.907500\n",
      "Loss: 0.393965, Train accuracy: 0.915000, val accuracy: 0.905000\n",
      "Loss: 0.286251, Train accuracy: 0.920000, val accuracy: 0.905000\n",
      "Loss: 0.226309, Train accuracy: 0.925000, val accuracy: 0.907500\n",
      "Loss: 0.226982, Train accuracy: 0.922500, val accuracy: 0.912500\n",
      "Loss: 0.419131, Train accuracy: 0.922500, val accuracy: 0.920000\n",
      "Loss: 0.257593, Train accuracy: 0.922500, val accuracy: 0.915000\n",
      "Loss: 0.117727, Train accuracy: 0.922500, val accuracy: 0.917500\n",
      "Loss: 0.432678, Train accuracy: 0.932500, val accuracy: 0.920000\n",
      "Loss: 0.391083, Train accuracy: 0.935000, val accuracy: 0.925000\n",
      "Loss: 0.350247, Train accuracy: 0.932500, val accuracy: 0.925000\n",
      "Loss: 0.283513, Train accuracy: 0.932500, val accuracy: 0.925000\n",
      "Loss: 0.278191, Train accuracy: 0.932500, val accuracy: 0.925000\n",
      "Loss: 0.263409, Train accuracy: 0.932500, val accuracy: 0.927500\n",
      "Loss: 0.305113, Train accuracy: 0.932500, val accuracy: 0.925000\n",
      "Loss: 0.173224, Train accuracy: 0.932500, val accuracy: 0.925000\n",
      "Loss: 0.345833, Train accuracy: 0.932500, val accuracy: 0.927500\n",
      "Loss: 0.349722, Train accuracy: 0.932500, val accuracy: 0.927500\n",
      "Loss: 0.310194, Train accuracy: 0.932500, val accuracy: 0.927500\n",
      "Loss: 0.240363, Train accuracy: 0.932500, val accuracy: 0.927500\n",
      "Loss: 0.241243, Train accuracy: 0.932500, val accuracy: 0.932500\n",
      "Loss: 0.321072, Train accuracy: 0.932500, val accuracy: 0.930000\n",
      "Loss: 0.186258, Train accuracy: 0.932500, val accuracy: 0.930000\n",
      "Loss: 0.480210, Train accuracy: 0.930000, val accuracy: 0.935000\n",
      "Loss: 0.184439, Train accuracy: 0.930000, val accuracy: 0.937500\n",
      "Loss: 0.206171, Train accuracy: 0.932500, val accuracy: 0.937500\n",
      "Loss: 0.401381, Train accuracy: 0.932500, val accuracy: 0.937500\n",
      "Loss: 0.296127, Train accuracy: 0.932500, val accuracy: 0.937500\n",
      "Loss: 0.190252, Train accuracy: 0.932500, val accuracy: 0.937500\n",
      "Loss: 0.380238, Train accuracy: 0.937500, val accuracy: 0.937500\n",
      "Loss: 0.280001, Train accuracy: 0.937500, val accuracy: 0.937500\n",
      "Loss: 0.195664, Train accuracy: 0.937500, val accuracy: 0.937500\n",
      "Loss: 0.154557, Train accuracy: 0.937500, val accuracy: 0.937500\n",
      "Loss: 0.319246, Train accuracy: 0.937500, val accuracy: 0.937500\n",
      "Loss: 0.105498, Train accuracy: 0.937500, val accuracy: 0.937500\n",
      "Loss: 0.476771, Train accuracy: 0.937500, val accuracy: 0.942500\n",
      "Loss: 0.244298, Train accuracy: 0.937500, val accuracy: 0.942500\n",
      "Loss: 0.218440, Train accuracy: 0.940000, val accuracy: 0.942500\n",
      "Loss: 0.166159, Train accuracy: 0.937500, val accuracy: 0.940000\n",
      "Loss: 0.404824, Train accuracy: 0.937500, val accuracy: 0.942500\n",
      "Loss: 0.459622, Train accuracy: 0.937500, val accuracy: 0.945000\n",
      "Loss: 0.181082, Train accuracy: 0.937500, val accuracy: 0.945000\n",
      "Loss: 0.260295, Train accuracy: 0.937500, val accuracy: 0.945000\n",
      "Loss: 0.222647, Train accuracy: 0.937500, val accuracy: 0.945000\n",
      "Loss: 0.343900, Train accuracy: 0.937500, val accuracy: 0.942500\n",
      "Loss: 0.236053, Train accuracy: 0.940000, val accuracy: 0.942500\n",
      "Loss: 0.163574, Train accuracy: 0.940000, val accuracy: 0.942500\n",
      "Loss: 0.277819, Train accuracy: 0.940000, val accuracy: 0.942500\n",
      "Loss: 0.311227, Train accuracy: 0.942500, val accuracy: 0.945000\n",
      "Loss: 0.207586, Train accuracy: 0.940000, val accuracy: 0.945000\n",
      "Loss: 0.392662, Train accuracy: 0.940000, val accuracy: 0.945000\n",
      "Loss: 0.168316, Train accuracy: 0.940000, val accuracy: 0.945000\n",
      "Loss: 0.185936, Train accuracy: 0.940000, val accuracy: 0.945000\n",
      "Loss: 0.345145, Train accuracy: 0.940000, val accuracy: 0.945000\n",
      "Loss: 0.121601, Train accuracy: 0.940000, val accuracy: 0.945000\n",
      "Loss: 0.092310, Train accuracy: 0.940000, val accuracy: 0.945000\n",
      "Loss: 0.225802, Train accuracy: 0.940000, val accuracy: 0.945000\n",
      "Loss: 0.259897, Train accuracy: 0.940000, val accuracy: 0.945000\n",
      "Loss: 0.501867, Train accuracy: 0.940000, val accuracy: 0.947500\n",
      "Loss: 0.127620, Train accuracy: 0.940000, val accuracy: 0.947500\n",
      "Loss: 0.111019, Train accuracy: 0.940000, val accuracy: 0.945000\n",
      "Loss: 0.138374, Train accuracy: 0.940000, val accuracy: 0.945000\n",
      "Loss: 0.207826, Train accuracy: 0.940000, val accuracy: 0.945000\n",
      "Loss: 0.080008, Train accuracy: 0.940000, val accuracy: 0.945000\n",
      "Loss: 0.205767, Train accuracy: 0.940000, val accuracy: 0.945000\n",
      "Accuracy 0.945\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "model = MLPClassifier([\n",
    "    Linear(2, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 3)\n",
    "])\n",
    "dataset = Dataset(X, y, X_test, y_test);\n",
    "trainer = Trainer(model, dataset, SGD(), num_epochs=100, batch_size=64,\n",
    "                  learning_rate=5e-2, learning_rate_decay=0.99);\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "best_acc = max(np.mean(model.predict(X_test) == y_test), best_acc)\n",
    "print(\"Accuracy\", best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPbVTFnMQuvW"
   },
   "source": [
    "## PyTorch\n",
    "\n",
    "Для выполнения следующего задания понадобится PyTorch. [Инструкция по установке](https://pytorch.org/get-started/locally/)\n",
    "\n",
    "Если у вас нет GPU, то можно использовать [Google Colab](https://colab.research.google.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "tV0mJLu-QuvX"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "VUC_QqpAQuva"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "t = transforms.ToTensor()\n",
    "\n",
    "cifar_train = datasets.CIFAR10(\"datasets/cifar10\", download=True, train=True, transform=t)\n",
    "train_loader = DataLoader(cifar_train, batch_size=1024, shuffle=True, pin_memory=torch.cuda.is_available())\n",
    "cifar_test = datasets.CIFAR10(\"datasets/cifar10\", download=True, train=False, transform=t)\n",
    "test_loader = DataLoader(cifar_test, batch_size=1024, shuffle=False, pin_memory=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGmpjcFfQuvd"
   },
   "source": [
    "### Задание 4 (3 балла)\n",
    "А теперь поработам с настоящими нейронными сетями и настоящими данными. Необходимо реализовать сверточную нейронную сеть, которая будет классифицировать изображения из датасета CIFAR10. Имплементируйте класс `Model` и функцию `calculate_loss`. \n",
    "\n",
    "Обратите внимание, что `Model` должна считать в конце `softmax`, т.к. мы решаем задачу классификации. Соответствеено, функция `calculate_loss` считает cross-entropy.\n",
    "\n",
    "Для успешного выполнения задания необходимо, чтобы `accuracy`, `mean precision` и `mean recall` были больше 0.5\n",
    "\n",
    "__Можно пользоваться всем содержимым библиотеки PyTorch.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flattener(nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size, *_ = x.shape\n",
    "        return x.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "5sRmTKwKQuve"
   },
   "outputs": [],
   "source": [
    "nn_model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(4),    \n",
    "            Flattener(),\n",
    "            nn.Linear(64*2*2, 10),\n",
    "          )\n",
    "nn_model.type(torch.cuda.FloatTensor)\n",
    "\n",
    "def calculate_loss(X: torch.Tensor, y: torch.Tensor, model: torch.nn):\n",
    "    \"\"\"\n",
    "    Cчитает cross-entropy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch.Tensor\n",
    "        Данные для обучения.\n",
    "    y : torch.Tensor\n",
    "        Метки классов.\n",
    "    model : Model\n",
    "        Модель, которую будем обучать.\n",
    "\n",
    "    \"\"\"\n",
    "    loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "    return loss(model.forward(X), y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAsLmkUqQuvh"
   },
   "source": [
    "Теперь обучим нашу модель. Для этого используем ранее созданные batch loader'ы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "k5G8iMCeQuvh"
   },
   "outputs": [],
   "source": [
    "def train(model, epochs=20):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for i in range(epochs):\n",
    "        #Train\n",
    "        loss_mean = 0\n",
    "        elements = 0\n",
    "        for X, y in iter(train_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            loss = calculate_loss(X, y, model)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_mean += loss.item() * len(X)\n",
    "            elements += len(X)\n",
    "        train_losses.append(loss_mean / elements)\n",
    "        #Test\n",
    "        loss_mean = 0 \n",
    "        elements = 0\n",
    "        for X, y in iter(test_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            loss = calculate_loss(X, y, model)\n",
    "            loss_mean += loss.item() * len(X)\n",
    "            elements += len(X)\n",
    "        test_losses.append(loss_mean / elements)\n",
    "        print(\"Epoch\", i, \"| Train loss\", train_losses[-1], \"| Test loss\", test_losses[-1])\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "vmD9eWJOQuvl",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train loss 2.0831674994659424 | Test loss 1.8290363525390625\n",
      "Epoch 1 | Train loss 1.7390293963241577 | Test loss 1.6492141250610353\n",
      "Epoch 2 | Train loss 1.590431149520874 | Test loss 1.5179573949813843\n",
      "Epoch 3 | Train loss 1.4846123386383057 | Test loss 1.457369924545288\n",
      "Epoch 4 | Train loss 1.4091246754837037 | Test loss 1.3883760877609252\n",
      "Epoch 5 | Train loss 1.356443648147583 | Test loss 1.3290432334899902\n",
      "Epoch 6 | Train loss 1.3083702619171143 | Test loss 1.2982218536376953\n",
      "Epoch 7 | Train loss 1.2722885869979859 | Test loss 1.25388401927948\n",
      "Epoch 8 | Train loss 1.2477298760604858 | Test loss 1.2554021125793458\n",
      "Epoch 9 | Train loss 1.2245239672088624 | Test loss 1.2226422241210937\n",
      "Epoch 10 | Train loss 1.1951482787322998 | Test loss 1.1953569398880004\n",
      "Epoch 11 | Train loss 1.1702125289535523 | Test loss 1.1872159814834595\n",
      "Epoch 12 | Train loss 1.1548093476867676 | Test loss 1.1597872409820558\n",
      "Epoch 13 | Train loss 1.1443623762893678 | Test loss 1.1469871904373168\n",
      "Epoch 14 | Train loss 1.1253728946304322 | Test loss 1.1296281240463257\n",
      "Epoch 15 | Train loss 1.1111196193695068 | Test loss 1.1272163919448852\n",
      "Epoch 16 | Train loss 1.0936592505645752 | Test loss 1.1027584548950196\n",
      "Epoch 17 | Train loss 1.0800909384918214 | Test loss 1.1008335699081422\n",
      "Epoch 18 | Train loss 1.0676693186187745 | Test loss 1.0917079259872438\n",
      "Epoch 19 | Train loss 1.0558695875167847 | Test loss 1.075316876220703\n"
     ]
    }
   ],
   "source": [
    "train_l, test_l = train(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJNAuHjNQuvn"
   },
   "source": [
    "Построим график функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "F6OEGqriQuvo"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(train_l)), train_l, label=\"train\")\n",
    "plt.plot(range(len(test_l)), test_l, label=\"test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miUxg0bDQuvs"
   },
   "source": [
    "И, наконец, посчитаем метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "UXSOJFI8Quvt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy 0.6282\n",
      "Precision [0.68061224 0.73653846 0.48732943 0.48181818 0.57052632 0.51969981\n",
      " 0.69325736 0.68036072 0.7549121  0.64608696]\n",
      "Recall [0.667 0.766 0.5   0.371 0.542 0.554 0.73  0.679 0.73  0.743]\n",
      "Mean Precision 0.6251141588289434\n",
      "Mean Recall 0.6282\n"
     ]
    }
   ],
   "source": [
    "true_positive = np.zeros(10)\n",
    "true_negative = np.zeros(10)\n",
    "false_positive = np.zeros(10)\n",
    "false_negative = np.zeros(10)\n",
    "accuracy = 0\n",
    "ctn = 0\n",
    "for X, y in iter(test_loader):\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_pred = nn_model(X).max(dim=1)[1]\n",
    "    for i in range(10):\n",
    "        for pred, real in zip(y_pred, y):\n",
    "            if real == i:\n",
    "                if pred == real:\n",
    "                    true_positive[i] += 1\n",
    "                else:\n",
    "                    false_negative[i] += 1\n",
    "            else:\n",
    "                if pred == i:\n",
    "                    false_positive[i] += 1\n",
    "                else:\n",
    "                    true_negative[i] += 1\n",
    "            \n",
    "    accuracy += torch.sum(y_pred == y).item()\n",
    "    ctn += len(y)\n",
    "print(\"Overall accuracy\", accuracy / ctn)\n",
    "print(\"Precision\", true_positive / (true_positive + false_positive))\n",
    "print(\"Recall\", true_positive / (true_positive + false_negative))\n",
    "print(\"Mean Precision\", np.mean(true_positive / (true_positive + false_positive)))\n",
    "print(\"Mean Recall\", np.mean(true_positive / (true_positive + false_negative)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKA-j4rIQuvv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw05_task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
